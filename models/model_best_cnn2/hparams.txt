# added additional layer
# added batch normalization.

max_epochs: 15
batch_size: 3
learning_rate: 0.000203228544324115
weight_decay: 1.2697111322756407e-05
f1: 24
k1: 3
last test_loss: [0.00487431]
test_loss: [[0.01848459]
 [0.0110647 ]
 [0.00824173]
 [0.0071323 ]
 [0.00635096]
 [0.0059803 ]
 [0.00561424]
 [0.00539041]
 [0.00516368]
 [0.00534684]
 [0.00516776]
 [0.00492345]
 [0.00509603]
 [0.00482923]
 [0.00487431]]
training time: h:0 m:7 s:46
